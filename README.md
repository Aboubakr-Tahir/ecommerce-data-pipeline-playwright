# **ğŸ“¸ E-Commerce Data Pipeline with Playwright & MongoDB**

This project is a robust, production-ready web scraper designed to bypass Cloudflare's advanced bot detection and scrape product data from **B\&H Photo Video**.

It uses a **"Hybrid Architecture"**:

1. **Manual Authentication:** Uses your real installed browser (Edge/Chrome) to generate valid "Human" cookies.
2. **Automated Scraping:** Uses Playwright (masked as the real browser) to scrape thousands of pages using those cookies.
3. **Data Pipeline:** Extracts data instantly to **MongoDB** (via Docker) to prevent data loss during crashes.

## **ğŸ“‚ Project Structure**

bh_scraper_project/  
â”‚  
â”œâ”€â”€ data/ \# Stores your "VIP Pass" (Cookies)
â”‚ â””â”€â”€ auth.json \# (Generated by Script 1\)  
â”‚  
â”œâ”€â”€ docker/ \# Database Infrastructure  
â”‚ â””â”€â”€ docker-compose.yml \# MongoDB \+ Mongo Express UI  
â”‚  
â”œâ”€â”€ scripts/ \# Python Logic  
â”‚ â”œâ”€â”€ 1_get_access.py \# The "Key Maker" (Run manually)  
â”‚ â””â”€â”€ 2_scrape_data.py \# The "Robot" (Run automatically)  
â”‚  
â”œâ”€â”€ requirements.txt \# Python dependencies  
â””â”€â”€ .gitignore \# Security rules

## **ğŸ› ï¸ Prerequisites**

1. **Python 3.8+** installed.
2. **Docker Desktop** installed and running.
3. **Microsoft Edge** (or Chrome) installed on your Windows PC.

## **ğŸš€ Setup Guide (End-to-End)**

### **Step 1: Install Dependencies**

Open your terminal in the project root folder:

pip install \-r requirements.txt  
playwright install

### **Step 2: Start the Database (MongoDB)**

We use Docker to run the database so you don't have to install MongoDB manually.

1. Navigate to the docker folder: cd docker
2. Start the services:  
   docker-compose up \-d

3. **Verify:** Open your browser to [http://localhost:8081](https://www.google.com/search?q=http://localhost:8081).
   - **Username:** admin
   - **Password:** pass #if it do not work its password123
   - If you see the "Mongo Express" dashboard, you are ready.

### **Step 3: Generate the "VIP Key" (Auth)**

**This is the most important step.** Cloudflare blocks standard bots. We will use a script to launch your **REAL** Edge browser, let you solve the CAPTCHA manually, and then save the "Proof of Humanity" to a file.

1. Navigate to scripts: cd ../scripts (or open the scripts folder).
2. Run the access script:  
   python 1_get_access.py

3. **Action Required:**
   - A new Edge window will open.
   - If you see a "Verify you are human" box, **CLICK IT**.
   - Wait until you see the actual B\&H Product Page.
   - Go back to your terminal and press **ENTER**.
4. **Result:** A file named auth.json will appear in the data/ folder. This is your key.

### **Step 4: Run the Scraper**

Now that you have the key (auth.json) and the warehouse (MongoDB), release the robot.

python 2_scrape_data.py

- The script will launch Edge (visibly, to maintain the human fingerprint).
- It will automatically navigate, pagination, and extract data.
- Data is saved to MongoDB in real-time.

## **âš™ï¸ Configuration & Customization**

### **How to Scrape a Different Website (e.g., Amazon)**

You need to edit **2 files**:

1. **Edit scripts/1_get_access.py:**
   - Change TARGET_URL to a product page on the new site.
   - Run the script again to generate a new auth.json for that site.
2. **Edit scripts/2_scrape_data.py:**
   - Change body_url to the new category listing page.
   - **Update Selectors:** You must find the CSS selectors for the new site:
     - wait_for_selector(...): Find a unique element (like \#productTitle).
     - product_links: Update how to find the list of products.
     - target_data: Update the logic to find the JSON-LD schema or HTML elements.

### **Troubleshooting**

- **"Timeout 30000ms exceeded":** Cloudflare likely blocked you.
  - **Fix:** Run 1_get_access.py again to refresh your auth.json key.
- **"Redirected to base URL":** The scraper reached the last page of results.
- **MongoDB Error:** Ensure Docker is running (docker ps).

## **âš ï¸ Disclaimer**

This project is for **educational purposes only**. Scraping websites may violate their Terms of Service. Use responsibly and respect robots.txt.

## âš–ï¸ Legal & Ethical Disclaimer

This project is strictly for **educational and research purposes**. It demonstrates how to build robust web automation and data pipelines using modern tools.

**Important Considerations:**

1.  **Respect `robots.txt`:** Always check the target website's `robots.txt` file before scraping.
2.  **Rate Limiting:** This tool implements delays to avoid overwhelming servers. Do not remove these delays.
3.  **Terms of Service:** This code is a proof-of-concept. Using it against websites that prohibit scraping may violate their Terms of Service.
4.  **Personal Data:** This scraper is designed for public product data only. It does not and should not be used to collect personal user data.

**The author is not responsible for any misuse of this software.**
